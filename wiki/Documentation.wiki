#summary User Guide for ELSA
#labels Featured

=Introduction=
Enterprise Log Search and Archive is a solution to achieve the following:
 * Normalize, store, and index logs at unlimited volumes and rates
 * Provide a simple and clean search interface and API
 * Provide an infrastructure for alerting, reporting and sharing logs
 * Control user actions with local or LDAP/AD-based permissions
 * Plugin system for taking actions with logs
 * Exist as a completely free and open-source project

ELSA accomplishes these goals by harnessing the highly-specialized strengths of other open-source projects:  Perl provides the glue to asynchronously tie the log receiver (Syslog-NG) together with storage (MySQL) and indexing (Sphinx Search) and serves this over a web interface provided either by Apache or any other web server, including a standalone pure-Perl server for a lighter footprint.

==Why ELSA?==
I wrote ELSA because commercial tools were both lacking and cost prohibitive.  The only tool that provided the features I needed was Splunk.  Unfortunately, it was cost prohibitive and was too slow to receive the log volume I wanted on the hardware I had available.  ELSA is inspired by Splunk but is focused on speed versus dashboards and presentation.

In designing ELSA, I tried the following components but found them too slow.  Here they are ordered from fastest to slowest for indexing speeds (non-scientifically tested):
 # Tokyo Cabinet
 # MongoDB
 # TokuDB MySQL plugin
 # Elastic Search (Lucene)
 # Splunk
 # HBase
 # CouchDB
 # MySQL Fulltext

=Capabilities=

ELSA achieves _n_ node scalability by allowing every log receiving node to operate completely independently of the others.  Queries from a client through the API against the nodes are sent in parallel so the query will take only the amount of time the of the longest response.  Query results are aggregated by the API before being sent to the client as a response.  Response times vary depending on the number of query terms and their selectivity, but a given node on modest hardware takes about one half second per billion log entries.

Log reception rates greater than 50,000 events per second per node are achieved through the use of a fast pattern parser in Syslog-NG called PatternDB.  The pattern parser allows Syslog-NG to normalize logs without resorting to computationally expensive regular expressions.  This allows for sustained high log reception rates in Syslog-NG which are piped directly to a Perl program which further normalizes the logs and prepares large text files for batch inserting into MySQL.  MySQL is capable of inserting over 100,000 rows per second when batch loading like this.  After each batch is loaded, Sphinx indexes the newly inserted rows in temporary indexes, then again in larger batches every few hours in permanent indexes.  

Sphinx can create temporary indexes at a rate of 50,000 logs per second consolidate these temporary indexes at around 35,000 logs per second, which becomes the terminal sustained rate for a given node.  The effective bursting rate is around 100,000 logs per second, which is the upper bound of Syslog-NG on most platforms.  If indexing cannot keep up, a backlog of raw text files will accumulate.  In this way, peaks of several hours or more can be endured without log loss but with an indexing delay.

The overall flow diagram looks like this:

Live, continuously:

Network → Syslog-NG (PatternDB) → Raw text file

Batch load (by default every minute):

Raw text file → MySQL → Sphinx

=Plugins=
ELSA ships with several plugins:
 * Windows logs from [http://eventlog-to-syslog.googlecode.com Eventlog-to-Syslog]
 * Snort/Suricata logs
 * Bro logs
 * Url logs from [http://enterprise-log-search-and-archive.googlecode.com/files/httpry_logger.pl httpry_logger]
These plugins tell the web server what to do when a user clicks the "Info" link next to each log.  It can do anything, but it is designed for returning useful information in a dialog panel in ELSA with an actions menu.  An example that ships with ELSA is that if a [http://streamdb.googlecode.com StreamDB] URL is configured (or OpenFPC) any log that has an IP address in it will have a "getPcap" option which will autofill pcap request parameters for one-click access to the traffic related to the log being viewed.

New plugins can be added easily by subclassing the "Info" Perl class and editing the elsa_web.conf file to include them.  Contributions are welcome!

=File Locations=

The main ELSA configuration files are /etc/elsa_node.conf and /etc/elsa_web.conf.  All configuration is controlled through these files, except for query permissions which are stored in the database and administrated through the web interface.  Nodes read in the elsa_node.conf file every batch load, so changes may be made to it without having to restart Syslog-NG.

Most Linux distributions do not ship recent versions of Syslog-NG.  Therefore, the install compiles it from source and installs it to $BASE_DIR/syslog-ng with the configuration file in $BASE_DIR/syslog-ng/etc/, where it will be read by default.  By default, $BASE_DIR is /usr/local and $DATA_DIR is /data.  Syslog-NG writes raw files to $DATA_DIR/elsa/tmp/buffers/<random file name> and loads them into the index and archive tables at an interval configured in the elsa_node.conf file, which is 60 seconds by default.  The files are deleted upon successful load.  When the logs are bulk inserted into the database, Sphinx is called to index the new rows.  When indexing is complete, the loader notes the new index in the database which will make it available to the next query.  Indexes are stored in $DATA_DIR/sphinx and comprise about as much space as the raw data stored in MySQL.  

Archive tables typically compress at a 10:1 ratio, and therefore use only about 5% of the total space allocated to logs compared with the index tables and indexes themselves.  The index tables are necessary because Sphinx searches return only the ID's of the matching logs, not the logs themselves, therefore a primary key lookup is required to retrieve the raw log for display.  For this reason, archive tables alone are insufficient because they do not contain a primary key.

=Web Server=

The web frontend is typically served with Apache, but the Plack Perl module allows for any web server to be used, including a standalone server called Starman which can be downloaded from CPAN.  Any implementation will still have all authentication features available because they are implemented in the underlying Perl.

The server is backended on the ELSA web database, (elsa_web by default), which stores user information including permissions, query log, stored results, and query schedules for alerting.

Admins are designated by configuration variables in the elsa_web.conf file, either by system group when using local auth, or by LDAP/AD group when using LDAP auth.  To designate a group as an admin, add the group to the array in the configuration.  Under the “none” auth mode, all users are admins because they are all logged in under a single pseudo-username.  

=Configuration=

Most settings in the elsa_web.conf and elsa_node.conf files should be fine with the defaults, but there are a few important settings which need to be changed depending on the environment.
==elsa_web.conf:==
 * Nodes: Contains the connection information to the log node databases which hold the actual data.
 * Auth_method: Controls how authentication and authorization occurs.  For LDAP, the ldap settings must also be filled out.
 * Link_key: should be changed to something other than the default.  It is used to salt the auth hashes for permalinks.
 * Email: For alerts and archive query notifications, you need to setup the email server to use.
 * Meta_db: Should point to the database which stores the web management information.  This can reside on a node, but probably shouldn't.  The performance won't be much of a factor, so running this locally on the web server should be fine.
==elsa_node.conf:==
 * Database: Edit the connection settings for the local database, if non-default.
 * Log_size_limit: Total size in bytes allowed for all logs and indexes.
 * Sphinx/perm_index_size: This setting must be tweaked so that perm_index_size number of logs come into the system before (num_indexes `*` sphinx/index_interval) seconds pass.
 * Archive/percentage: Percentage of log_size_limit reserved for archive.
==Firewall Settings==
||Source||Destination||Port||
||Web Clients||Web Node||TCP 80/443||
||Web Node||LDAP/AD Server||TCP 389/636||
||Web Node||Log Node||TCP 3306||
||Web Node||Log Node||TCP 3307||
||Log Clients||Log Node||TCP/UDP 514||

=Permissions=

Log access is permitted by allowing certain groups either universal access (admins) or a whitelist of attributes.  The attributes can be log hosts (the hosts that initially generate the logs), ranges of hosts (by IP), log classes, or log nodes (the nodes that store the logs).  Groups can be either AD groups or local system groups, as per the configuration.  Those in the admins group have the "Admin" drop-down menu next to the "ELSA" drop down menu in the web interface which has a "Manage Permissions" item which opens a new window for administrating group privileges.

=Queries=

Query syntax is loosely based on Google search syntax.  Terms are searched as whole keywords (no wildcards).  Searches may contain boolean operations specifying that a term is required using the plus sign, negating using the minus sign, or no sign indicating that it is an “OR.”  Parenthesis may be used to group terms.  Numeric fields, including hosts, may have greater than or less than (and equal to) operators combined with the boolean operators.  Queries can be very simple, like looking for any mention of an IP addres:
{{{
10.0.20.1
}}}
Or a website
{{{
site:www.google.com
}}}
Here is an example query for finding Symantec Anti-Virus alerts on Windows logs on ten hosts that does not contain the keyword “TrackingCookie:”
{{{
+eventid:51 host>10.0.0.10 host<10.0.0.20 -TrackingCookie
}}}
One could also look for account lockouts that do not come from certain hosts:
{{{
+class:windows +locked -host>10.0.0.10 -host<10.0.0.20
}}}
To see what hosts have had lockout events, one could run:
{{{
+class:windows +”locked out”
}}}
and choose the ANY.host field from the “Report On” menu.

When the API issues the query, it is parsed and sent to Sphinx.  It then receives the log ID's that match and the API queries MySQL for those ID's:

Query → Parse → Authorize → Log → Sphinx → MySQL → Aggregation → Presentation

=Archive Queries=

Queries for logs in the archive tables take much longer than indexed queries.  For this reason, they are run in the background and the requester is notified via email when the query results are ready.  The results are viewed through the link in the email or through the web interface menu for “Saved Results.”  Archive queries are run exactly like normal queries except that the “Index” toggle button is changed to “Archive.”  They may be performed on the same time range available in the indexed logs as a way of performing wildcard searches not restricted to a keyword.  For example, if it was necessary to find a log matching a partial word, one could run an archive search with a narrow time selection.  A user may only run a single archive query at a time to prevent system overload.  In addition, there is a configuration variable specifying how many concurrent users may run an archive query (the default is four).  Most systems can search about 10 million logs per minute per node from the archive.  The overall flow looks like this:

Archive Query → Parse → Authorize → Log → Batch message to user
 				(then in background)	→ MySQL → Store in web MySQL → Email

=Alerts=

Any query that has been run may be turned into an alert by clicking the “Results...” menu button and choosing “alert.”  This will execute the exact same search after every new batch of logs is loaded, and will notify the user via email of new hits in a manner similar to the archive search results.

=Scheduled Queries=

Any query may be scheduled to run at a specified interval.  This can be useful for creating daily or hourly reports.  Creating the scheduled query is similar to creating the alert in that you choose the option from the “Results...” button after performing a search you wish to create a report from.

=Command-line Interface and API=

ELSA ships with a command-line interface, elsa/web/cli.pl, which can be run when logged in on the web frontend from the shell.  This can be helpful for testing or piping results to other programs.  However, the Perl API provides a much more comprehensive method for accessing ELSA in a scripted fashion.  You can use the included cli.pl as an example for using the API.

=Performance Tuning=
==Node==
===Hardware===
For very high logging levels, there are many factors which can affect overall throughput and search response times.  Hardware is a major factor, and disk speed is the biggest consideration.  ELSA should only ever use a maximum of three CPU's at a time (during index consolidation) and usually uses only one.  However, Sphinx is a threaded application, and will happily scale to as many CPU's are on the box for running queries in parallel.  For this reason, four CPU's is recommended for production systems with a high load.  RAM is also a factor, but less so.  2-4 GB should be enough.  Searchd, the Sphinx daemon, will consume most of the RAM with its temporary indexes.

===Index Configuration===
ELSA has two kinds of indexes, temporary and permanent.  Only temporary indexes allow attribute-only searches.  Attribute-only searches (searches based soley on time, class, program, or any range search) require storing the attributes in RAM, which will consume all RAM if all indexes were done that way for high log volumes.  In order to not have RAM be the limiting factor for ELSA, permanent indexes do not allow attribute-only searches.  Thus, when you search, for example, class=none, you're only searching temporary indexes, which is usually the last few minutes or hours, depending on when the last consolidation took place on the backend.

The best workaround is to query for host= where the host is a log host you know will have results.  You can do multiple hosts to get the same effect.  However, if you use any range queries like host>1.1.1.1 host<2.2.2.2, then host will become an attribute and you'll only get partial results again.

There is a way to get around this if you have enough RAM.  If you edit /usr/local/elsa/node/Indexer.pm in the "get_sphinx_conf" subroutine and change the text under "permanent" from "docinfo = inline" to "docinfo = external" and rename the sphinx config file in the /etc/elsa_node.conf to something new, it will create a new sphinx config file at that location.  Then, you will need to re-index everything:
{{{
service syslog-ng stop
mv /etc/elsa_node.conf /etc/elsa_node.conf.bak
# Edit Index.pm
perl /usr/local/elsa/node/elsa.pl -c /etc/elsa_node.conf -on
service searchd stop
# (This next step could take a really long time if you have a lot of logs loaded already)
/usr/local/sphinx/bin/indexer --rotate --all 
service searchd start
service syslog-ng start
}}}

However, this will only work if you have many gigs of RAM and not that many logs (less than a billion).  To get an idea of how much RAM would be required, you can sum these totals (assuming your Sphinx directory is /data/sphinx):
{{{
du -sch /data/sphinx/*.spa
du -sch /data/sphinx/*.spi
}}}
If this number fits comfortably in under your RAM totals, and you think your logs have rolled (as in, you don't expect much more than this), you could safely change that setting.

===Filesystem Considerations===
Ext4 is currently the default filesystem on most recent Linux distributions.  It is a happy medium between ReiserFS which is good for small files and XFS which excels at large files.  ELSA deals mostly with large files, so it is recommended to create a dedicated partition for your $DATA_DIR and format it with XFS, but ext4 should not hinder performance much.

===MySQL===
ELSA batch loads files using "LOAD DATA INFILE" and so it benefits from a few configuration file changes to enlarge the batch loading buffer:
{{{
[mysqld]
bulk_insert_buffer_size = 100M
}}}
In addition, MySQL 5.5 in general offers significant performance increases over previous versions in many areas, though those optimizations will be more apparent on the database serving the web interface versus the log node databases.

===VMware Considerations===
If you are running ELSA as a VM, you should set the disk to the "high" setting to get the best performance.

==Web==
The web server itself should not need any special performance tuning.  Web clients, however, are highly encouraged to use either Firefox or Chrome for browsers because of the heavy use of Javascript on the page.
=Monitoring=

You can use the "Stats" page under the "Admin" menu on the web interface to see what ELSA's usage looks like.  To diagnose problems, refer to the $DATA_DIR/elsa/log directory, especially the node.log and web.log files, respectively.

You may also want to look for network problems on nodes, especially kernel drops.  You can errors like this with this command:
{{{
netstat -s | grep -i errors
}}}
Look at the whole output of "netstat -s" for context if you see errors.

It may also be a good idea to establish a log that you know should periodically occur.  Then do a query on the web interface and report on a time value, such as hour or day, and look for any fluctuations in that value that could indicate log loss.

=Adding Parsers=
In order to add parsers, you need to add patterns to the patterndb.xml file.  If you need to create new log classes and fields, it's not too hard, but right now there is no web interface (that's planned in the future).  You'll need to add classes to the "classes" table, fields to the "fields" table, then use the offsets listed under $Field_order in web/lib/API.pm to create the right entries in "fields_classes_map."  Other than those few database entries, adding the pattern and restarting syslog-ng and apache is all you have to do.  The new fields
will show up in the web interface, etc.  If you can, try to create patterns which re-use existing classes and fields, then just dropping them into the patterndb.xml file will instantly make them parse correctly-no DB work or restarts needed.  I plan on making a blog post on how to do this soon, but let me know if you run into any troubles.  Here's an example to get you started:

Example log 
program: 
{{{test_prog}}}
message: 
{{{source_ip 1.1.1.1 sent 50 bytes to destination_ip 2.2.2.2 from user joe}}}
Pick a class_id greater than 10000 for your own custom classes.  Let's
say this is the first one, so your new class_id will be 10000.
What to insert into syslog database on log node:
{{{
INSERT INTO classes (class_id, class) VALUES (10000, "NEWCLASS");
}}}
Our fields will be conn_bytes, srcip, and dstip, which already exist
in the "fields" table as well as "myuser" which we will create here
for demonstration purposes:
{{{
INSERT INTO fields (field, field_type, pattern_type) VALUES ("myuser",
"string", "QSTRING");

INSERT INTO fields_classes_map (field_id, class_id, field_order)
VALUES ((SELECT class_id FROM classes WHERE class="NEWCLASS"), (SELECT
field_id FROM fields WHERE field="srcip"), 5);
INSERT INTO fields_classes_map (field_id, class_id, field_order)
VALUES ((SELECT class_id FROM classes WHERE class="NEWCLASS"), (SELECT
field_id FROM fields WHERE field="conn_bytes"), 6);
INSERT INTO fields_classes_map (field_id, class_id, field_order)
VALUES ((SELECT class_id FROM classes WHERE class="NEWCLASS"), (SELECT
field_id FROM fields WHERE field="dstip"), 7);
}}}
Now the string field "myuser" at field_order 11, which maps to the
first string column "s0":
{{{
INSERT INTO fields_classes_map (field_id, class_id, field_order)
VALUES ((SELECT class_id FROM classes WHERE class="NEWCLASS"), (SELECT
field_id FROM fields WHERE field="myuser"), 11);
}}}
5, 6, and 7 correspond to the first integer columns in the schema
"i0," "i1," and "i2."  In the pattern below, we're extracting the data
and calling it i0-i2 so that it goes into the log database correctly.
The above SQL maps the names of these fields in the context of this
class to those columns in the raw database when performing searches.

Example pattern:
{{{
<ruleset name="does_not_matter" id='does_not_matter_either'>
                <pattern>test_prog</pattern>
                <rules>
                        <rule provider="does_not_matter" class='21' id='21'>
                                <patterns>
                                        <pattern>source_ip @IPv4:i0:@ sent @ESTRING:i1: @bytes to destination_ip @IPv4:i2:@ from user @ANYSTRING:s0:@</pattern>
                                </patterns>
                                <examples>
                                  <example>
                                    <test_message program="test_prog">source_ip 1.1.1.1 sent 50 bytes to destination_ip 2.2.2.2 from user joe</test_mesage>
                                    <!-- srcip -->
                                    <test_value name="i0">1.1.1.1</test_value>
                                    <!-- conn_bytes -->
                                    <test_value name="i1">50</test_value>
                                    <!-- dstip -->
                                    <test_value name="i2">2.2.2.2</test_value>
                                    <!-- myuser -->
                                    <test_value name="s0">joe</test_value>
                                  </example>
                                </examples>
                        </rule>
                </rules>
        </ruleset>
}}}
Add this in the patterndb.xml between the <patterndb></patterndb>
elements.  You can test this on a log node using the
/usr/local/syslog-ng/bin/pdbtool utility like so:
{{{
/usr/local/syslog-ng/bin/pdbtool test -p /usr/local/elsa/node/conf/patterndb.xml
}}}
This should print out all of the correct test values.  You can test it against example messages as well like this:
{{{
/usr/local/syslog-ng/bin/pdbtool match -p /usr/local/elsa/node/conf/patterndb.xml -P test_prog -M "source_ip 1.1.1.1 sent 50 bytes to destination_ip 2.2.2.2 from user joe"
}}}

After the patterndb.xml file and the database are updated, you will need to restart syslog-ng:
{{{
service syslog-ng restart
}}}
If you are already logged into ELSA, simply refreshing the page should make those new classes and fields available.

=Transforms=
ELSA has a powerful feature called transforms which allow you to pass the results of a query to a backend plugin.  The plugins that currently ship with ELSA include whois, dnsdb, and CIF (Collective Intelligence Framework).  There are also utility transforms filter, grep, and sum.
==Syntax==
Transforms are modeled after UNIX-style command pipes, like this:
{{{
site:www.google.com | whois | sum(descr)
}}}
This command finds all URL requests for site www.google.com, passes those results to the whois plugin which attaches new fields like org and description, and then passes those results to the sum transform which takes the argument "descr" indicating which field to sum.  The result is a graph of the unique "descr" field as provided by the whois plugin.

Plugins take the syntactical form of:
{{{
query | plugin_1(arg1,arg2,argn) | plugin_n(arg1,arg2,argn)
}}}

==Current Plugins==
The currently shipped plugins are:
||Name||Args||Description||
||whois|| ||ARIN and RIPE online databases to add network owner info||
||dnsdb|| ||isc.dnsdb.org's database (if an api key is provided)||
||cif|| ||Queries a local Collective Intelligence Framework server||
||grep||regex on field, regex on value||Only passes results that match the test||
||filter||regex on field, regex on value||Only passes results that do not match the test||
||sum||field||Sums the total found for the given field||

=OSSEC Integration=
ELSA can read logs from OSSEC, here's how:
Edit /usr/local/syslog-ng/etc/syslog-ng.conf:
{{{
source s_ossec {
  file("/OSSEC_BASE_DIR/ossec/logs/archives/archives.log" program_override('ossec-archive') follow_freq(1) flags(no-parse));
}
log {
 source(s_ossec); destination(d_elsa);
};
}}}

To enable archive output in OSSEC, add to ossec.conf file <global> section:
{{{
<logall>yes</logall>
}}}